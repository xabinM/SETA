import os
import time
import json
import logging
from datetime import datetime, timezone

from app.models import RoomSummaryState, PromptBuilt, TokenUsage
from app.adapters.kafka_io import make_consumer, make_producer, publish, read_headers
from app.utils.trace import extract_traceparent
from app.adapters.db import get_session
from app.services import prompt_builder_service, llm_client, error_service
from app.adapters.redis_io import append_conversation
from app.utils.usage import estimate_usage_by_tokens


logging.basicConfig(
    level=logging.DEBUG,   # INFO â†’ DEBUG ë¡œ ë³€ê²½
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s"
)
logger = logging.getLogger("llm-worker")

# ElasticSearch, huggingface, httpx ë‚´ë¶€ ë¡œê·¸ ê°ì¶”ê¸°
logging.getLogger("elastic_transport.transport").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("tokenizers").setLevel(logging.WARNING)

# huggingface tokenizers warning ì œê±°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

KAFKA_IN = os.getenv("KAFKA_TOPIC_IN_LLM", "chat.filter.result.v1")
KAFKA_OUT_DELTA = os.getenv("KAFKA_TOPIC_OUT_LLM_DELTA", "chat.llm.answer.delta.v1")
KAFKA_OUT_DONE = os.getenv("KAFKA_TOPIC_OUT_LLM_DONE", "chat.llm.answer.done.v1")


def log_llm_process(user_input: str, system_prompt: str, context_snippets: list,
                    similar_contexts: list, full_text: str = None, usage: dict = None):
    try:
        lines = []
        lines.append("ğŸ¤– [LLM ì²˜ë¦¬ ê³¼ì • ìš”ì•½]")

        lines.append(f"  ğŸ“ ìœ ì € ì…ë ¥: \"{user_input}\"")

        lines.append("  âš™ï¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸:")
        for sp_line in system_prompt.splitlines():
            lines.append(f"    {sp_line}")

        if context_snippets:
            lines.append("  ğŸ’¬ ìµœê·¼ ëŒ€í™” ë§¥ë½:")
            for i, ctx in enumerate(context_snippets, 1):
                lines.append(f"    {i}) {ctx}")
        else:
            lines.append("  ğŸ’¬ ìµœê·¼ ëŒ€í™” ë§¥ë½ ì—†ìŒ")

        if similar_contexts:
            lines.append("  ğŸ” ìœ ì‚¬ ë§¥ë½(ES):")
            for i, ctx in enumerate(similar_contexts, 1):
                if isinstance(ctx, dict):
                    text = ctx.get("text", "")
                    score = ctx.get("score", 0)
                    preview = text[:100] + "..." if len(text) > 100 else text
                    lines.append(f"    {i}) (ì ìˆ˜={score:.2f}) {preview}")
                else:
                    preview = ctx[:100] + "..." if len(ctx) > 100 else ctx
                    lines.append(f"    {i}) {preview}")
        else:
            lines.append("  ğŸ” ìœ ì‚¬ ë§¥ë½ ì—†ìŒ")

        if full_text is not None:
            lines.append(f"  âœ… LLM ìµœì¢… ë‹µë³€: {full_text[:100]}{'...' if len(full_text) > 100 else ''}")

        if usage:
            lines.append(
                f"  ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: í”„ë¡¬í”„íŠ¸={usage.get('prompt_tokens', 0)}, "
                f"ì™„ì„±={usage.get('completion_tokens', 0)}, ì´í•©={usage.get('total_tokens', 0)}"
            )

        logger.info("\n" + "\n".join(lines))

    except Exception as e:
        logger.warning("âš ï¸ ë¡œê·¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜: %s", e)


def run_worker():
    consumer = make_consumer([KAFKA_IN], group_id="llm-worker")
    producer = make_producer()
    logger.info("ğŸš€ llm-worker started (IN=%s, OUT_DELTA=%s, OUT_DONE=%s)", KAFKA_IN, KAFKA_OUT_DELTA, KAFKA_OUT_DONE)

    while True:
        msg = consumer.poll(1.0)
        if msg is None:
            logger.debug("â³ no message polled")
            continue
        if msg.error():
            logger.error("âŒ Kafka ì˜¤ë¥˜: %s", msg.error())
            continue

        try:
            logger.debug("ğŸ“© Raw Kafka message: %s", msg.value())
            ev = json.loads(msg.value().decode("utf-8"))
            logger.info("ğŸ“¥ Kafka ë©”ì‹œì§€ ë””ì½”ë”© ì„±ê³µ: %s", ev)
        except Exception as e:
            logger.error("âŒ Kafka ë©”ì‹œì§€ ë””ì½”ë”© ì‹¤íŒ¨: %s", e)
            continue

        headers_dict = read_headers(msg)
        tp = extract_traceparent(headers_dict)
        logger.debug("traceparent=%s", tp)

        decision = ev.get("decision") or {}
        action = decision.get("action") or ev.get("action")
        logger.debug("decision=%s, action=%s", decision, action)
        if action != "PASS":
            logger.info("â© PASSê°€ ì•„ë‹Œ ë©”ì‹œì§€ ê±´ë„ˆëœ€ (action=%s)", action)
            continue

        trace_id = ev.get("trace_id")
        chat_room_id = ev.get("room_id")
        message_id = ev.get("message_id")
        user_id = ev.get("user_id")
        user_id = int(user_id) if user_id is not None else None

        logger.debug("â–¶ï¸ trace_id=%s, room_id=%s, message_id=%s, user_id=%s", trace_id, chat_room_id, message_id, user_id)

        # ì…ë ¥ í…ìŠ¤íŠ¸ í™•ë³´
        user_input = ev.get("cleaned_text") or ev.get("original_text") or ""
        logger.debug("user_input=%s", user_input)

        try:
            with get_session() as session:
                # 1) system_prompt
                system_prompt = prompt_builder_service.build_system_prompt(session, user_id)
                system_prompt += "\n\në‹µë³€ì€ ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
                logger.debug("system_prompt=%s", system_prompt)

                # 2) ìµœê·¼ ëŒ€í™” ë§¥ë½
                context_snippets = [
                    f"{m['role']}: {m['content']}"
                    for m in prompt_builder_service.get_recent_conversation(chat_room_id, limit=10)
                ]
                logger.debug("context_snippets=%s", context_snippets)

                # 3) ES embedding ê¸°ë°˜ ê²€ìƒ‰
                similar_contexts = prompt_builder_service.search_similar_context_es(
                    query=user_input, user_id=user_id, top_k=3, min_score=0.7
                )
                logger.debug("similar_contexts=%s", similar_contexts)

                # 4) full_prompt ì¡°ë¦½
                full_prompt = (
                    f"System: {system_prompt}\n\n"
                    + "\n".join(context_snippets)
                    + ("\n\n[ê³¼ê±° ìœ ì‚¬ ë§¥ë½]\n" + "\n".join(similar_contexts) if similar_contexts else "")
                    + (f"\n\nìœ ì €: {user_input}" if user_input else "")
                )
                logger.debug("full_prompt=%s", full_prompt)

                # 5) PromptBuilt ì €ì¥
                pb = PromptBuilt(
                    trace_id=trace_id,
                    built_prompt=full_prompt,
                    context_messages=context_snippets,
                    created_at=datetime.now(timezone.utc),
                )
                session.add(pb)
                session.commit()
                logger.debug("PromptBuilt ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            logger.exception("âŒ PROMPT_BUILD_ERROR")
            error_service.save_error(trace_id=trace_id, error_type="PROMPT_BUILD_ERROR", error=e)
            continue

        start = time.time()
        model_name = os.getenv("LLM_MODEL", "gpt-4.1-nano")
        temperature = float(os.getenv("LLM_TEMPERATURE", "0.7"))
        logger.info("ğŸ¤– LLM í˜¸ì¶œ ì¤€ë¹„: model=%s, temperature=%s", model_name, temperature)

        chunks = []
        try:
            for event in llm_client.call_llm(full_prompt, stream=True, model=model_name, temperature=temperature):
                logger.info(f"LLM Raw Event: {event}")
                if event["type"] == "delta":
                    delta = event["delta"]
                    chunks.append(delta)
                    logger.debug("delta=%s", delta)

                    try:
                        payload = {
                            "trace_id": trace_id,
                            "room_id": chat_room_id,
                            "message_id": message_id,
                            "delta": delta,
                            "timestamp": int(datetime.now(timezone.utc).timestamp() * 1000),
                        }
                        logger.debug("ğŸ“¤ Publish DELTA payload=%s", payload)
                        publish(
                            producer,
                            KAFKA_OUT_DELTA,
                            key=chat_room_id,
                            value=payload,
                            headers=[("traceparent", tp.encode())] if tp else None,
                        )
                    except Exception as e:
                        logger.exception("ğŸ”¥ LLM ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                        error_service.save_error(trace_id, "KAFKA_DELTA_ERROR", e)

                elif event["type"] == "done":
                    usage = event["usage"]
                    latency_ms = int((time.time() - start) * 1000)
                    full_text = "".join(chunks)

                    log_llm_process(user_input, system_prompt, context_snippets, similar_contexts, full_text, usage)

                    # TokenUsage ì €ì¥
                    try:
                        with get_session() as session:
                            total_tokens = usage.get("total_tokens", 0)
                            cost_usd, energy_wh, co2_g, water_ml = estimate_usage_by_tokens(total_tokens)

                            token_usage = TokenUsage(
                                message_id=message_id,
                                user_id=user_id,
                                prompt_tokens=usage.get("prompt_tokens", 0),
                                completion_tokens=usage.get("completion_tokens", 0),
                                total_tokens=total_tokens,
                                cost_usd=cost_usd,
                                energy_wh=energy_wh,
                                co2_g=co2_g,
                                saved_tokens=0,
                                saved_cost_usd=0,
                                saved_energy_wh=0,
                                saved_co2_g=0,
                                created_at=datetime.now(timezone.utc),
                            )
                            session.add(token_usage)
                            session.commit()
                            logger.debug("TokenUsage ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        logger.exception("âŒ DB_INSERT_ERROR (TokenUsage)")
                        error_service.save_error(trace_id, "DB_INSERT_ERROR", e)

                    # Redis Append (user + assistant ëŒ€í™” ì €ì¥)
                    try:
                        append_conversation(room_id=chat_room_id, role="user", content=user_input)
                        append_conversation(room_id=chat_room_id, role="assistant", content=full_text)
                        logger.debug("Redis Append ì™„ë£Œ")
                    except Exception as e:
                        logger.exception("âŒ REDIS_APPEND_ERROR")
                        error_service.save_error(trace_id, "REDIS_APPEND_ERROR", e)

                    # unsummarized_count++
                    try:
                        with get_session() as session:
                            state = session.query(RoomSummaryState).filter_by(chat_room_id=chat_room_id).first()
                            if state:
                                state.unsummarized_count = (state.unsummarized_count or 0) + 1
                                if state.last_summary_at is None:
                                    state.last_summary_at = datetime.now(timezone.utc)
                                session.commit()
                                logger.debug("RoomSummaryState ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    except Exception as e:
                        logger.exception("âŒ DB_UPDATE_ERROR (RoomSummaryState)")
                        error_service.save_error(trace_id, "DB_UPDATE_ERROR", e)

                    # Kafka DONE ë°œí–‰
                    try:
                        payload = {
                            "trace_id": trace_id,
                            "room_id": chat_room_id,
                            "message_id": message_id,
                            "response": {"text": full_text},
                            "usage": usage,
                            "latency_ms": latency_ms,
                            "schema_version": "1.0.0",
                            "timestamp": int(datetime.now(timezone.utc).timestamp() * 1000),
                        }
                        logger.debug("ğŸ“¤ Publish DONE payload=%s", payload)
                        publish(
                            producer,
                            KAFKA_OUT_DONE,
                            key=chat_room_id,
                            value=payload,
                            headers=[("traceparent", tp.encode())] if tp else None,
                        )

                        done_at = int(datetime.now(timezone.utc).timestamp() * 1000)
                        produced_at = int(ev.get("timestamp", done_at))
                        total_pipeline_ms = done_at - produced_at
                        logger.info("\n" + f"ğŸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì‹œê°„ (LLM DONE): {total_pipeline_ms}ms")

                    except Exception as e:
                        logger.exception("âŒ KAFKA_DONE_ERROR")
                        error_service.save_error(trace_id, "KAFKA_DONE_ERROR", e)

        except Exception as e:
            logger.exception("âŒ LLM_CALL_ERROR")
            error_service.save_error(trace_id, "LLM_CALL_ERROR", e)


if __name__ == "__main__":
    run_worker()
