services:

  # Spark Master 
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
    ports:
      - '8081:8080'
    networks:
      - app-network

  # Spark Worker
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    mem_limit: 4g
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
    networks:
      - app-network
    depends_on:
      - spark-master

  # Spark Streaming Application
  spark-streaming-app:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-streaming-app-new
    mem_limit: 4g
    ports:
      - "4042:4040"
      - "4043:4041"
    working_dir: /opt/bitnami/spark/app
    volumes:
      - ./stream_filter_router.py:/opt/bitnami/spark/app/stream_filter_router.py
      - ./filter_word.json:/opt/bitnami/spark/app/filter_word.json
    command: /bin/bash -c "sleep 5 && /opt/bitnami/spark/bin/spark-submit --master spark://spark-master:7077 --driver-memory 2g --executor-memory 2g --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 --conf \"spark.driver.extraJavaOptions=-Dfile.encoding=UTF-8\" --conf \"spark.executor.extraJavaOptions=-Dfile.encoding=UTF-8\" --conf spark.driver.port=4040 --conf spark.ui.port=4041 /opt/bitnami/spark/app/stream_filter_router.py"
    environment:
      # Kafka 연결
      - KAFKA_BOOTSTRAP_SERVERS=3.35.206.91:29092
      - INPUT_TOPIC=chat.raw.request.v1
      - OUTPUT_TO_KAFKA=true
      - OUTPUT_TOPIC=chat.raw.filtered.v1
      - SPARK_DRIVER_HOST=spark-streaming-app-new
      - LANG=en_US.UTF-8
      - LC_ALL=en_US.UTF-8
      - PYTHONIOENCODING=UTF-8
      - JAEGER_AGENT_HOST=3.35.206.91
      - JAEGER_AGENT_PORT=6831
    networks:
      - app-network
    depends_on:
      - spark-worker

  # Spark Batch Application
  spark-batch:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-batch-processor
    volumes:
      - ./spark_batch_aggregation.py:/opt/bitnami/spark/app/spark_batch_aggregation.py
    environment:
      # PostgreSQL 연결
      - POSTGRES_HOST=${POSTGRES_HOST}
      - POSTGRES_PORT=${POSTGRES_PORT}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    command: tail -f /dev/null
    networks:
      - app-network

networks:
  app-network:
    driver: bridge